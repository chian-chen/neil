{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "edf81abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "066d478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(index, p=7):\n",
    "    # feature_points = np.array([9, 29])\n",
    "    # illuminant = np.load(f'illuminant_features/{p}/{index}.npy')\n",
    "    # R = illuminant[:, 0][feature_points]\n",
    "    # G = illuminant[:, 1][feature_points]\n",
    "    # B = illuminant[:, 2][feature_points]\n",
    "    \n",
    "    # feature1 = R[1] - R[0]\n",
    "    # feature2 = G[1] - G[0]\n",
    "    # feature3 = np.std(B)\n",
    "    # feature4 = feature1 * feature2 * feature3\n",
    "\n",
    "    feature_points = np.arange(0, 40, 1)\n",
    "    feature_vector = np.load(f'illuminant_features/{index}.npy').sum(axis=1)[feature_points]\n",
    "    feature1 = np.mean(feature_vector)\n",
    "    feature2 = np.std(feature_vector)\n",
    "    feature3 = np.max(feature_vector)\n",
    "    feature4 = np.min(feature_vector)\n",
    "    \n",
    "    return np.array([feature1, feature2, feature3, feature4])\n",
    "\n",
    "def get_p_feature_vector(index):\n",
    "    # feature_points = np.array([9, 29])\n",
    "    feature_vector = []\n",
    "    for p in [1, 3, 7, 10]:\n",
    "        # 0: sigma = 0.5, 1: sigma = 1, 2:sigma = 1.5\n",
    "        feature_vector.append(np.load(f'illuminant_features/{p}/{index}.npy').sum(axis=1)[1])\n",
    "\n",
    "    feature_vector = np.array(feature_vector)\n",
    "    feature1 = np.mean(feature_vector)\n",
    "    feature2 = np.var(feature_vector)\n",
    "    feature3 = np.max(feature_vector)\n",
    "    feature4 = np.min(feature_vector)\n",
    "    return np.array([feature1, feature2, feature3, feature4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b34b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('arr_reference.json', 'r') as f:\n",
    "    ref = json.load(f)\n",
    "    \n",
    "others = ref['others']\n",
    "worst25 = ref['worst25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "47e3cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_point_discriminability(data_all: np.ndarray,\n",
    "                                  data_worst: np.ndarray,\n",
    "                                  feature_nums: np.ndarray,\n",
    "                                  output_dir: str = None) -> list:\n",
    "    \"\"\"\n",
    "    Evaluate and visualize univariate discriminability at each sigma.\n",
    "\n",
    "    For each sigma value, this function:\n",
    "      1. Computes Cohen's d between `data_all` and `data_worst` at that sigma.\n",
    "      2. Performs an independent t-test (unequal variance) and returns the p-value.\n",
    "      3. Computes ROC AUC treating `data_worst` as the positive class.\n",
    "      4. Plots overlaid histograms of the two distributions with annotations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_all : np.ndarray, shape (n_all, n_sigmas)\n",
    "        Feature values for the entire dataset across different sigma values.\n",
    "    data_worst : np.ndarray, shape (n_worst, n_sigmas)\n",
    "        Feature values for the worst-25% subset across the same sigma values.\n",
    "    sigmas : np.ndarray, shape (n_sigmas,)\n",
    "        The sigma values corresponding to the columns of data_all and data_worst.\n",
    "    output_dir : str, optional\n",
    "        Directory path to save each histogram plot. If None, plots display interactively.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics : list of dicts\n",
    "        A list where each entry corresponds to one sigma and contains:\n",
    "          - 'sigma': the sigma value\n",
    "          - 'cohen_d': Cohen's d effect size\n",
    "          - 'p_value': two-sided p-value from t-test\n",
    "          - 'auc': ROC AUC score\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> metrics = single_point_discriminability(\n",
    "    ...     data_all=all_feats,              # shape (513, 21)\n",
    "    ...     data_worst=worst25_feats,         # shape (128, 21)\n",
    "    ...     sigmas=np.linspace(0, 2, 21),\n",
    "    ...     output_dir='figures')\n",
    "    \"\"\"\n",
    "    n_all, _ = data_all.shape\n",
    "    n_worst, _ = data_worst.shape\n",
    "    combined = np.vstack([data_all, data_worst])\n",
    "    labels = np.concatenate([np.zeros(n_all), np.ones(n_worst)])\n",
    "\n",
    "    metrics = []\n",
    "    os.makedirs(output_dir, exist_ok=True) if output_dir else None\n",
    "\n",
    "    for idx, feature_nums in enumerate(feature_nums):\n",
    "        x_all = data_all[:, idx]\n",
    "        x_worst = data_worst[:, idx]\n",
    "\n",
    "        # Cohen's d\n",
    "        var_all = x_all.var(ddof=1)\n",
    "        var_worst = x_worst.var(ddof=1)\n",
    "        pooled_sd = np.sqrt(((n_all - 1)*var_all + (n_worst - 1)*var_worst) / (n_all + n_worst - 2))\n",
    "        cohen_d = (x_all.mean() - x_worst.mean()) / pooled_sd\n",
    "\n",
    "        # t-test (unequal variance)\n",
    "        t_stat, p_value = ttest_ind(x_all, x_worst, equal_var=False)\n",
    "\n",
    "        # AUC (worst25 as positive class)\n",
    "        auc = roc_auc_score(labels, combined[:, idx])\n",
    "\n",
    "        metrics.append({\n",
    "            'feature_nums': feature_nums,\n",
    "            'cohen_d': cohen_d,\n",
    "            'p_value': p_value,\n",
    "            'auc': auc\n",
    "        })\n",
    "\n",
    "        # Plot distributions\n",
    "        plt.figure()\n",
    "        plt.hist(x_all, bins=30, alpha=0.5, label='all')\n",
    "        plt.hist(x_worst, bins=30, alpha=0.5, label='worst25')\n",
    "        plt.title(f\"sigma={feature_nums:.2f} | d={cohen_d:.2f}, p={p_value:.3f}, AUC={auc:.3f}\")\n",
    "        plt.xlabel('Illuminant feature')\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend()\n",
    "\n",
    "        if output_dir:\n",
    "            fname = os.path.join(output_dir, f\"feature_nums_{feature_nums:.2f}.png\")\n",
    "            plt.savefig(fname, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b7af2099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36098/3940850306.py:58: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cohen_d = (x_all.mean() - x_worst.mean()) / pooled_sd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'feature_nums': 1,\n",
       "  'cohen_d': -0.35440858236572526,\n",
       "  'p_value': 0.0006502859954185577,\n",
       "  'auc': 0.6197037337662338},\n",
       " {'feature_nums': 2, 'cohen_d': nan, 'p_value': nan, 'auc': 0.5},\n",
       " {'feature_nums': 3,\n",
       "  'cohen_d': -0.35440858236572526,\n",
       "  'p_value': 0.0006502859954185577,\n",
       "  'auc': 0.6197037337662338},\n",
       " {'feature_nums': 4,\n",
       "  'cohen_d': -0.35440858236572526,\n",
       "  'p_value': 0.0006502859954185577,\n",
       "  'auc': 0.6197037337662338}]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_features = []\n",
    "for i in others:\n",
    "    feature_vector = get_p_feature_vector(i)\n",
    "    other_features.append(feature_vector)\n",
    "    \n",
    "worst25_features = []\n",
    "for i in worst25:\n",
    "    feature_vector = get_p_feature_vector(i)\n",
    "    worst25_features.append(feature_vector)\n",
    "\n",
    "os.makedirs(f'./figs/', exist_ok=True)\n",
    "single_point_discriminability(\n",
    "        data_all=np.array(other_features),\n",
    "        data_worst=np.array(worst25_features),\n",
    "        feature_nums=np.array([1, 2, 3, 4]),\n",
    "        output_dir=f'./figs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "498fbb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'feature_nums': 1,\n",
       "  'cohen_d': -0.37928431009039354,\n",
       "  'p_value': 0.006882092089552756,\n",
       "  'auc': 0.6261831848552338},\n",
       " {'feature_nums': 2,\n",
       "  'cohen_d': -0.6139777283160801,\n",
       "  'p_value': 0.0017935657631015255,\n",
       "  'auc': 0.644244153674833},\n",
       " {'feature_nums': 3,\n",
       "  'cohen_d': -0.4455512168017853,\n",
       "  'p_value': 0.0013626844883303298,\n",
       "  'auc': 0.6508560690423162},\n",
       " {'feature_nums': 4,\n",
       "  'cohen_d': -0.21828466907754995,\n",
       "  'p_value': 0.11812817279781472,\n",
       "  'auc': 0.5626739977728284}]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_features = []\n",
    "for i in others:\n",
    "    feature_vector = get_feature_vector(i, p=7)\n",
    "    other_features.append(feature_vector)\n",
    "    \n",
    "worst25_features = []\n",
    "for i in worst25:\n",
    "    feature_vector = get_feature_vector(i, p=7)\n",
    "    worst25_features.append(feature_vector)\n",
    "\n",
    "single_point_discriminability(\n",
    "        data_all=np.array(other_features),\n",
    "        data_worst=np.array(worst25_features),\n",
    "        feature_nums=np.array([1, 2, 3, 4]),\n",
    "        output_dir=f'./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c0f6e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2d_classifier(data_all: np.ndarray,\n",
    "                        data_worst: np.ndarray,\n",
    "                        sigmas: np.ndarray,\n",
    "                        sigma_pair: tuple,\n",
    "                        test_size: float = 0.2,\n",
    "                        random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Train a 2D logistic regression classifier using two sigma features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_all : np.ndarray, shape (n_all, n_sigmas)\n",
    "    data_worst : np.ndarray, shape (n_worst, n_sigmas)\n",
    "    sigmas : np.ndarray, shape (n_sigmas,)\n",
    "    sigma_pair : tuple of two floats\n",
    "        The sigma values to use as the 2D features.\n",
    "    test_size : float\n",
    "        Proportion of data to use for test split.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : sklearn.linear_model.LogisticRegression\n",
    "        Trained logistic regression model.\n",
    "    indices : tuple of two ints\n",
    "        Column indices of the chosen sigma values.\n",
    "    accuracy : float\n",
    "        Classification accuracy on the test split.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> model, indices, acc = train_2d_classifier(\n",
    "    ...     all_feats, worst_feats,\n",
    "    ...     sigmas=np.linspace(0,2,21),\n",
    "    ...     sigma_pair=(0.5, 1.5)\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    # map sigma values to nearest indices\n",
    "    # idx1 = int(np.argmin(np.abs(sigmas - sigma_pair[0])))\n",
    "    # idx2 = int(np.argmin(np.abs(sigmas - sigma_pair[1])))\n",
    "\n",
    "    # X_all = data_all[:, [idx1, idx2]]\n",
    "    # X_worst = data_worst[:, [idx1, idx2]]\n",
    "    X_all = data_all\n",
    "    X_worst = data_worst\n",
    "\n",
    "    X = np.vstack([X_all, X_worst])\n",
    "    y = np.concatenate([np.zeros(len(X_all)), np.ones(len(X_worst))])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    print(f\"2D classifier test accuracy: {accuracy:.3f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def classify_point(model, point: tuple) -> tuple:\n",
    "    \"\"\"\n",
    "    Classify a single 2D point using the trained model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : trained sklearn classifier\n",
    "    point : tuple of two floats\n",
    "        Feature values corresponding to the chosen sigma pair.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    label : int\n",
    "        Predicted class (0 for all, 1 for worst25).\n",
    "    prob : float\n",
    "        Predicted probability of the positive class (worst25).\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> label, prob = classify_point(model, (0.545, 0.544))\n",
    "    \"\"\"\n",
    "    prob = model.predict_proba([point])[0, 1]\n",
    "    label = int(model.predict([point])[0])\n",
    "    return label, prob\n",
    "\n",
    "\n",
    "def plot_decision_boundary(model,\n",
    "                           data_all: np.ndarray,\n",
    "                           data_worst: np.ndarray,\n",
    "                           sigmas: np.ndarray,\n",
    "                           sigma_pair: tuple,\n",
    "                           grid_steps: int = 200,\n",
    "                           output_path: str = None):\n",
    "    \"\"\"\n",
    "    Plot decision boundary of the 2D classifier with scatter data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : trained sklearn classifier\n",
    "    data_all : np.ndarray, shape (n_all, n_sigmas)\n",
    "    data_worst : np.ndarray, shape (n_worst, n_sigmas)\n",
    "    sigmas : np.ndarray, shape (n_sigmas,)\n",
    "    sigma_pair : tuple of two floats\n",
    "    grid_steps : int\n",
    "        Resolution of the meshgrid.\n",
    "    output_path : str, optional\n",
    "        If provided, save the figure; otherwise show interactively.\n",
    "    \"\"\"\n",
    "    idx1 = int(np.argmin(np.abs(sigmas - sigma_pair[0])))\n",
    "    idx2 = int(np.argmin(np.abs(sigmas - sigma_pair[1])))\n",
    "\n",
    "    X1_all = data_all[:, idx1]\n",
    "    X2_all = data_all[:, idx2]\n",
    "    X1_w = data_worst[:, idx1]\n",
    "    X2_w = data_worst[:, idx2]\n",
    "\n",
    "    # create meshgrid\n",
    "    x_min = min(X1_all.min(), X1_w.min())\n",
    "    x_max = max(X1_all.max(), X1_w.max())\n",
    "    y_min = min(X2_all.min(), X2_w.min())\n",
    "    y_max = max(X2_all.max(), X2_w.max())\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, grid_steps),\n",
    "        np.linspace(y_min, y_max, grid_steps)\n",
    "    )\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    Z = model.predict(grid).reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X1_all, X2_all, label='all', s=20, edgecolor='k')\n",
    "    plt.scatter(X1_w, X2_w, label='worst25', s=20, edgecolor='k')\n",
    "    plt.xlabel(f'Feature at σ={sigmas[idx1]:.2f}')\n",
    "    plt.ylabel(f'Feature at σ={sigmas[idx2]:.2f}')\n",
    "    plt.legend()\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
